## EPITA Paris - NLP Lab 2 - Advanced Text Processing and Machine Translation

This repository houses the materials and exercises for Lab 2 of the Natural Language Processing (NLP) course at EPITA Paris. This lab delves into advanced text processing techniques and explores neural machine translation (NMT).

**Lab Focus**

This lab tackles four key areas:

**Part 1: Language Detection**

* This section explores methods for automatically identifying the language of a given text sentence.
* You will experiment with techniques to determine the most likely language of the input text.

**Part 2: Multilingual Embeddings**

* This section introduces the concept of multilingual embeddings, which represent words in multiple languages within a unified semantic space.
* You will explore approaches for aligning word embeddings across different languages, enabling semantic similarity comparisons between languages.

**Part 3: Transformer Self-Attention**

* This section delves into the mathematical underpinnings of Transformer models, a prevalent architecture in NLP. 
* We will focus on the concept of self-attention, a core component of Transformers, and explore its role in understanding relationships between words within a sentence.

**Part 4: Neural Machine Translation (NMT)**

* This section introduces neural machine translation, a deep learning technique for translating text from one language to another.
* We will explore the basic principles of NMT models and gain insights into how they function in a real-world application.

**Getting Started**

* Clone this repository to your local machine.
* Ensure you have Python and the required libraries installed (refer to the provided instructions).
* Follow the instructions in each part of the lab to complete the exercises.

**Lab Deliverables**

* Complete the provided Jupyter Notebooks for each section.
* Answer the questions posed throughout the lab exercises.

**Learning Outcomes**

By completing this lab, you will gain a deeper understanding of:

* Techniques for language identification of text data.
* Multilingual word embeddings and their role in cross-lingual tasks.
* The mathematical concepts behind Transformer models, specifically self-attention.
* The core principles of neural machine translation and its applications.

**Additional Notes**

* This lab builds upon the concepts covered in Lab 1.
* Resources and references for further exploration will be provided throughout the lab.
* Feel free to ask questions or seek clarification during the lab session.
